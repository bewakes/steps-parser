\chapter{Scope of the Project}
Parsing is the basic part of Natural Language Processing. While there are
applications which do not make use of word dependency relations and parse
trees, I believe making use of parse trees helpes in creating more robust
systems. Such systems find their application in question answering, chatbots,
information retrieval, sentences generation, speech processing, machine
translation and grammar checkers. Dependency parsing takes us one step closer
to Natural Language Understanding(NLU). The result is something that is
understandable and interpretable by both humans and computers. And this is a
very important thing because it allows collaboration of both humans and
computers to create more powerful and useful systems.
\newline
\newline
At the most basic level, the dependency parser allows to create \textbf{grammar
checkers}. A grammatically valid sentence will be parsed by the framework while
an incorrect one will not be. Grammar checkers can be used by writers, authors
and everyone who needs to communicate in Nepali through digital medium. This
will be helpful to Nepali linguists as well.
\newline
\newline
Since the parses encode the semantic structure as trees, this immensely helps
in \textbf{machine translation} systems. It is obvious that valid sentences can
be generated using parse trees. At the semantic level, a given sentence should
have similar structure in almost any language. Thus the task of machine
translation, on a simplified level, just requires the node by node translation of
words or phrases. This can also supplement the existing neural translation
systems which have difficulty when the sentence is too long. To be able to
capture long sentences as well, the sizes of neural translators also need to
increase which increases the model complexity in a polynomial manner.
\newline
\newline
The parse trees will be useful in \textbf{speech and sentence generation} as
well. Speech generation sounds trivial, but it is very hard to get the human
level accent because different parts of sentencs heed to have different volume
an pitch while delivering a speech. Such information can directly or
indirectly be encoded in the parse trees.
\newline
\newline
This work paves the pathway to creating \textbf{knowledge graphs} which allow, for
example, to remember and store the information as we do in our brains. Such
graph would contain processed infromation from natural language texts, say for
example wikipedia or news article. With compact and comprehensible
representation of unstructured data, it opens up lots of possibilities for a
truly intelligent agents. Although deep neural networks and similar models are
providing the state-of-the art results, they still are black boxes as we cannot
say what exactly is going within them and why they made the decision they made
for a task. Due to this very reason, we still need to be reluctant on using
such ``black-boxes'' on sensitive domains like finance and medicine. Knowledge
graphs will not solve such problems right away, but they are essential to head
towards ``interpretable intelligent systems''.
\newline
\newline
And then we can immediately have information retrieval and question answering
systems which can find use in almost every public service systems. Take for
example, government offices, police statiions, counters and other public serice
areas. We can compile all the basic information in text, feed to the
intelligent system and then the system is ready. We just need to provide
appropriate interface to the end users.
\newline
\newline
The immediate scope of dependency parsers might delusionally seem very narrow,
but once we consider the chain-reaction effect on the subsequent applications
these parsers can lead, there is virtually no limit in the long run that
intelligent systems won't be able to do.


